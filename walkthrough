
# cis655-final-project
Book Recommendation Generator using Google Cloud

# Activate Google Books API
1. In Google Cloud go to "API" > "Library" > search for "Books API" > enable
2. Navigate to "API" > "Credentials" > "Create Credentials" > name the key "books api key" > restrict key to "books api" > create key.
3. Save the books api key for later.

# Use Google Colab to Generate a Sample Dataset
1. Use the API key you just created to create a csv file of books. The code I used gathers information on books for Kindergarten through 8th grade. Note that there is a query limit of 40 queries per minute (I set a delay of 100 seconds between grades) and a limit of 1000 queries per day.
2. After running the code, download the csv to your computer

```
see Google Colab file
```

# Upload the csv file to a Cloud Storage bucket 
1. "Cloud Storage" > "Buckets" > "Create Bucket" > name your bucket, set a region, i made mine private
2. click on your bucket > "Upload" > "select files" > select your csv file

# Create a SQL Instance
1. "SQL" > "Intances" > "Create Instance" > name your instance (ex. book-recs-db) and select cheapest options. I enabled private (create a vpc to go with it, name it books-vpc, save the password you create) and public connectivity

# Connect SQL Instance to your bucket and get your csv file
1. Open CloudShell Terminal

1. start CloudProxy, it should return "Listening on...". If you see this, keep this open and open another tab. You will do this anytime you want to connect the session.
```
./cloud-sql-proxy book-recommendations-456120:us-central1:book-recs-db
#you can copy your connection info by clicking on instance and finding "connection name"
```

2. connect to PostgreSQL
```
psql -h 127.0.0.1 -U postgres -p 5432
```

3. create a new database
```
CREATE DATABASE book_recommendations_db;
```

4. connect to the database
```
\c book_recommendations_db
```

5. create a table. You may need to do this line by line. Type each line and hit enter. When finished, close the parentheses (see below) and hit enter.
```
CREATE TABLE books (
    id SERIAL PRIMARY KEY,
    title VARCHAR(255),
    authors VARCHAR(500),
    publisher VARCHAR(255),
    published_date VARCHAR(20),
    description TEXT,
    isbn_10 VARCHAR(20),
    isbn_13 VARCHAR(20),
    reading_mode_text VARCHAR(50),
    reading_mode_image VARCHAR(50),
    page_count INTEGER,
    categories VARCHAR(500),
    image_small VARCHAR(255),
    image_large VARCHAR(255),
    language VARCHAR(50),
    sale_country VARCHAR(50),
    list_price_amount DECIMAL(10, 2),
    list_price_currency VARCHAR(10),
    buy_link TEXT,
    web_reader_link TEXT,
    embeddable BOOLEAN,
    grade VARCHAR(10)
);
```


7.  import data from bucket to sql
```
gcloud sql import csv book-recommendations-456120:us-central1:book-recs-db gs://k8-books-bucket/k8_books_40.csv \
  --database=book_recommendations_db \
  --table=books
```

8. create a directory
```
sudo mkdir /cloudsql
sudo chown $USER /cloudsql
```

9. re-run proxy
```
./cloud-sql-proxy \
  --unix-socket /cloudsql \
  book-recommendations-456120:us-central1:book-recs-db
```

10. reconnect to database (may new tab or terminal)
```
psql -h /cloudsql/book-recommendations-456120:us-central1:book-recs-db \
     -U postgres -d book_recommendations_db
```

11. copy csv and include headers; single line of code
```
\copy books(title,authors,publisher,published_date,description,isbn_10,isbn_13,reading_mode_text,reading_mode_image,page_count,categories,maturity_rating,image_small,image_large,language,sale_country,list_price_amount,list_price_currency,buy_link,web_reader_link,embeddable,grade) FROM 'k8_books_40.csv' WITH CSV HEADER;
```

# Create Backend API
1. Open Cloudshell terminal

2. create a folder and change to that directory
```
mkdir book-api && cd book-api
```

3. create a virtual environment
```
python3 -m venv venv
source venv/bin/activate
```

4. install dependencies in shell
```
pip install flask psycopg2-binary flask_sqlalchemy python-dotenv fastapi uvicorn
```

5. create a .env file for your db (if this doesn't connect correctly, add to app.py code)
```
DB_USER=postgres
DB_PASSWORD=stimac-cis655-final
DB_NAME=book_recommendations_db
DB_HOST=127.0.0.1
```

6. check the variables are set, this should return the info you just entered in the last step
```
echo $DB_NAME
echo $DB_USER
echo $DB_PASSWORD
echo $DB_HOST
```

## `books` and app.py
7. click "Editor" button, navigate to books-api and create a new file named app.py
```
app.py files from files
```

8. *run the cloud sql proxy, it should return "Listening on...". If you see this, keep this open and open another tab. You will do this anytime you want to connect the session.*
```
./cloud-sql-proxy book-recommendations-456120:us-central1:book-recs-db
```

10. *ensure that you are in the right directory and activate the virtual environment (do this every new session)*
```
cd ~/book-api
source venv/bin/activate
```

12. click the "Web Preview" button and select "preview on port 8080"
13. If this gives an error, click on the url and add /docs to the end and enter
    orginal url: "https://8080-cs-399637968661-default.cs-us-central1-pits.cloudshell.dev/?authuser=0"
    new url: "https://8080-cs-399637968661-default.cs-us-central1-pits.cloudshell.dev/docs"

15. connect to database
```
psql "host=127.0.0.1 dbname=book_recommendations_db user=postgres password=stimac-cis655-final"
```
## `user` and `user_books`
16. create `user` and `user_books` table
```
CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE user_books (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(user_id),
    title TEXT,
    author TEXT,
    rating INTEGER CHECK (rating >= 1 AND rating <= 5),
    read_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

17. insert some sample data
```
INSERT INTO users (username, email) VALUES
('alice', 'alice@example.com'),
('bob', 'bob@example.com');

INSERT INTO user_books (user_id, title, author, rating) VALUES
(1, 'Charlotte‚Äôs Web', 'E. B. White', 5),
(1, 'Matilda', 'Roald Dahl', 4),
(2, 'The Giver', 'Lois Lowry', 5),
(2, 'The Hobbit', 'J.R.R. Tolkien', 3);
```


# merge books and user_books
```
# make sure ./cloud-sql-proxy book-recommendations-456120:us-central1:book-recs-db is still running in other tab

# connect to the database
psql "host=127.0.0.1 dbname=book_recommendations_db user=postgres password=stimac-cis655-final"
```

20. update `user_books` (if needed)
```
ALTER TABLE user_books
ADD COLUMN status TEXT,
ADD COLUMN progress INTEGER,
ADD COLUMN notes TEXT,
ADD COLUMN isbn_10 TEXT;
```


23. enable an extension to help with fuzzy matching, so that book titles and authors will match even if there are capitalization or minor spelling errors
```
CREATE EXTENSION IF NOT EXISTS pg_trgm;
``` 


26. update tables so that users can add books and ratings for books that arent already in the table. I did not include title and author in this update because we are keeping them as required.
```
sql "host=127.0.0.1 dbname=book_recommendations_db user=postgres password=stimac-cis655-final"

ALTER TABLE books
    ALTER COLUMN published_date   DROP NOT NULL,
    ALTER COLUMN description      DROP NOT NULL,
    ALTER COLUMN categories       DROP NOT NULL,
    ALTER COLUMN image_small      DROP NOT NULL,
    ALTER COLUMN image_large      DROP NOT NULL,
    ALTER COLUMN language         DROP NOT NULL,
    ALTER COLUMN sale_country     DROP NOT NULL,
    ALTER COLUMN grade            DROP NOT NULL;
```




30. added another books csv to make the recommendations run better
```
# copy the csv from the bucket to the shell
gsutil cp gs://k8-books-bucket/books.csv .

# open the db
psql "host=127.0.0.1 dbname=book_recommendations_db user=postgres password=stimac-cis655-final"

# in the db, import the data from the new csv
\copy books(
    title,authors,publisher,published_date,description,
    isbn_10,isbn_13,reading_mode_text,reading_mode_image,
    page_count,categories,image_small,image_large,language,
    sale_country,list_price_amount,list_price_currency,
    buy_link,web_reader_link,embeddable,grade
) FROM 'books.csv'
  WITH (FORMAT csv, HEADER true, NULL 'NA');
```

31. still in the db, alter the user table to include a grade level for the user
```
# alter user table to add grade
ALTER TABLE users
ADD COLUMN grade TEXT;
ALTER TABLE

# update the current users
UPDATE users SET grade = '3' WHERE username = 'alice';
UPDATE users SET grade = '4' WHERE username = 'bob_smith';
UPDATE users SET grade = '5' WHERE username = 'hstimac';
```


# Connect Vertext AI and BigQuery 
1. enable services in the shell terminal
```
gcloud services enable \
    aiplatform.googleapis.com \
    bigquery.googleapis.com \
    eventarc.googleapis.com \
    pubsub.googleapis.com \
    cloudfunctions.googleapis.com
```

2. set up BigQuery
```
# create a dataset
bq mk book_recs

# create embeddings table
bq query --use_legacy_sql=false """
CREATE OR REPLACE TABLE book_recs.book_embeddings (
  isbn_10 STRING,
  title STRING,
  embedding ARRAY<FLOAT64>
)
"""
```

3. create a pub/sub topic for new books and for rating books
```
gcloud pubsub topics create new-book
gcloud pubsub topics create rate-book
```


5. make a new directory and folder
```
mkdir ~/rate-book-function
cd ~/rate-book-function
```

7. create main.py in the rate-book-function
```
insert file
```

8. create a requirements.txt in the rate-book-function
```
insert file
```

9. deploy the function
```
gcloud functions deploy rate-book-writer \
  --runtime python311 \
  --entry-point entry_point \
  --trigger-topic rate-book \
  --region us-central1 \
  --memory 512MB \
  --timeout 60s \
  --source . \
  --no-gen2 \
  --set-env-vars GCP_PROJECT=book-recommendations-456120
```

10. test the function
```
gcloud pubsub topics publish rate-book \
  --message='{
    "user_id": 1,
    "title": "Charlotte‚Äôs Web",
    "author": "E. B. White",
    "isbn_10": "0064400557",
    "rating": 5,
    "status": "finished",
    "progress": 100,
    "notes": "So sweet!"
  }'
```


14. install pub/sub if not installed in shell
```
pip install google-cloud-pubsub
```


15. create embeddings for Vertex AI
```
# enable Vertext AI in the shell termnial
gcloud services enable aiplatform.googleapis.com

# install the SDK in the terminal
pip install google-cloud-aiplatform
```


17. open editor and create file generate_embeddings.py inside book-api
```
insert file
```

19. in console, click Navigation >> Vertex AI >> Dashboard >> enable all recommended APIs

20. run in shell, make sure you're in correct directory
```
python generate_embeddings.py
```


23. click Navigation >> BigQuery >> Studio >> book-recommendations-456120 (project name)

24. press the blue plus sign or the SQL query button to open a blank query page

25. enter the below code and press RUN. 
```
insert saved query (named mine bigquery sql template)
```


27. run in shell terminal
```
# make sure in right directory and in virtual environment
cd ~/book-api
source venv/bin/activate

# install if not already installed
pip install google-cloud-bigquery

# reload the app
uvicorn app:app --host=0.0.0.0 --port=8080 --reload

```


29. update BigQuery book_embeddings table to fix an error: Navigation >> BigQuery>> Studio >> select project name >> book_recs >> book_embeddings >> click Edit Schema button >> make sure you have title, author, grade, isbn_10 all as string nullable and embeddings as float repeated

30. export/extract more variables to enhance recommendations
```
# export 
gcloud sql export csv book-recs-db \
  gs://k8-books-bucket/book.csv \
  --database=book_recommendations_db \
  --query="SELECT isbn_10, title, authors, grade, description FROM books WHERE isbn_10 IS NOT NULL AND title IS NOT NULL" \
  --offload

# if denied, grant permission, find email
gcloud sql instances describe book-recs-db \
  --format="value(serviceAccountEmailAddress)"

# then grant permission 
gsutil iam ch \
  serviceAccount:p968113828557-ubbsf9@gcp-sa-cloud-sql.iam.gserviceaccount.com:roles/storage.objectAdmin \
  gs://k8-books-bucket

# then rerun export command
```

31.  import/update BigQuery
```
bq load \
  --source_format=CSV \
  --skip_leading_rows=1 \
  book_recs.book_embeddings \
  gs://k8-books-bucket/book.csv \
  isbn_10:STRING,title:STRING,authors:STRING,grade:STRING,description:STRING
```




38. update it in shell
```
bq update \
  --table \
  book-recommendations-456120:book_recs.book_embeddings \
  book_embeddings_schema.json
```

# Create the Vertex AI chatbox
1. install in shell if not already installed
```
pip install google-cloud-aiplatform
```

2. in Editor, create a chatbot.py file in book-api
```
# chatbot.py
import vertexai
from vertexai.language_models import ChatModel, InputOutputTextPair

vertexai.init(
    project="book-recommendations-456120", 
    location="us-central1"
)

chat_model = ChatModel.from_pretrained("chat-bison@002")

chat = chat_model.start_chat(
    context="You are a friendly book assistant. Recommend books based on what the user says. Use genres, topics, and reading level if given.",
)

def ask_chatbot(prompt: str) -> str:
    response = chat.send_message(prompt)
    return response.text
```

3. add to app.py
```
## AI Chatbot

from chatbot import ask_chatbot
from fastapi import Body

@app.post("/chat")
def chat_with_user(message: str = Body(..., embed=True)):
    try:
        response = ask_chatbot(message)
        return {"response": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

4. reload the app
```
uvicorn app:app --host=0.0.0.0 --port=8080 --reload
```

5. PAUSE: ran into issue with vertex ai, awaiting feeback

#####

# Add feedback/reaction endpoint for users interacting with recommendations
1. switch to db
```
psql -h 127.0.0.1 -U postgres -d book_recommendations_db
```

2. create table for feedback
```
CREATE TABLE recommendation_feedback (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(user_id),
    isbn_10 TEXT,
    feedback TEXT CHECK (feedback IN ('like', 'dislike', 'interested')),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```


4. update book_embeddings_schema.json
```
[
    {"name": "isbn_10", "type": "STRING"},
    {"name": "title", "type": "STRING"},
    {"name": "authors", "type": "STRING"},
    {"name": "description", "type": "STRING"},
    {"name": "grade", "type": "STRING"},
    {"name": "embedding", "type": "FLOAT64", "mode": "REPEATED"},
    {"name": "thumbs_up", "type": "INTEGER"},
    {"name": "thumbs_down", "type": "INTEGER"}
  ]
```

5. update bigquery table in shell
```
bq update \
  --table book-recommendations-456120:book_recs.book_embeddings \
  ./book_embeddings_schema.json
```


8. reload the app in shell
```
uvicorn app:app --host=0.0.0.0 --port=8080 --reload
```

# Build the Front End

1. make a static folder inside book-api
```
mkdir static
```

2. in Editor, select static folder and create file index.html

3. inside index.html add code
```
insert index.html code from files
```

5. rerun app
```
uvicorn app:app --host=0.0.0.0 --port=8080 --reload
```
 
# Cloud Run

1. in book-api in shell, copy all current dependencies to a new file (it will create the file)
```
pip freeze > requirements.txt
```

2. create a Docker file
```
# enter into shell
touch Dockerfile
```

3. open Editor, select Dockerfile, add code
```
# Use official Python image
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

# Copy rest of the app (code, templates, etc.)
COPY . .

# Expose port
EXPOSE 8080

# Run the FastAPI app with uvicorn
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8080"]
```

4. use shell terminal to deploy to Cloud Run
```
gcloud builds submit --tag gcr.io/book-recommendations-456120/book-api
gcloud run deploy book-api \
  --image gcr.io/book-recommendations-456120/book-api \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

# Build user authenticaion and better security

1. add to requirements.txt
```
python-jose
passlib[bcrypt]
```

2. run in shell
```
pip install -r requirements.txt
```

3. in Editor, create auth.py in book-api for user authentication

4. put the code in auth.py to handle registration, login, and utilities for users
```
insert auth.py from files
```

5. update `user` table
```
psql "host=127.0.0.1 dbname=book_recommendations_db user=postgres password=stimac-cis655-final"

ALTER TABLE users
ADD COLUMN password TEXT,
ADD COLUMN hashed_password TEXT;
```







10. redeploy to cloudrun to check functionality
```
# save to Docker file
gcloud builds submit --tag gcr.io/book-recommendations-456120/book-api

# redeploy Cloud Run
gcloud run deploy book-api \
  --image gcr.io/book-recommendations-456120/book-api \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --port 8080

```

# pivoting to using Cloud Composer instaed of Vertex AI

1. take Google Colab book api code and put it in file, adapting to use bucket. fetch_books_dag.py
```
insert fetch_books_dag.py file
```

2. grant role in shell
```
gcloud projects add-iam-policy-binding book-recommendations-456120 \
  --member="serviceAccount:service-968113828557@cloudcomposer-accounts.iam.gserviceaccount.com" \
  --role="roles/composer.ServiceAgentV2Ext"
```

3. create Composer environment
```
# takes awhile to work (up to 30 min ish)
gcloud composer environments create book-data-pipeline \
  --location=us-central1 \
  --image-version=composer-2.12.0-airflow-2.10.2 \
  --environment-size=small
```

4. when done, find bucket name
```
gcloud composer environments describe book-data-pipeline \
  --location=us-central1 \
  --format="value(config.dagGcsPrefix)"

# example output (use in next line):
# gs://us-central1-book-data-pipel-122da488-bucket/dags
```

5. upload DAG to Composer
```
gsutil cp fetch_books_dag.py gs://us-central1-book-data-pipel-122da488-bucket/dags
```

6. Run DAG: search Cloud Composer >> Environment >> select the right environemnt >> click AirFlow UI >> find your DAG (takes a coupke minutes to show up >> make sure toggled on >> click play button to run

7. clean up the csv by creating file clean_books_csv.py
```
insert clean_books_csv.py
```

8.
```
# in shell
python3 clean_books_csv.py

# confirm in bucket
gsutil ls gs://k8-books-bucket/k8_books_clean.csv
```

8.  enable service agent
```
gcloud services enable sqladmin.googleapis.com

# create an agent
gcloud iam service-accounts create cloudsql-uploader \
  --description="Used by Cloud SQL to read CSVs" \
  --display-name="Cloud SQL Uploader"

# grant permission
gsutil iam ch \
  serviceAccount:cloudsql-uploader@book-recommendations-456120.iam.gserviceaccount.com:objectViewer \
  gs://k8-books-bucket

```

9.  import to SQL
```
gcloud sql import csv book-recs-db \
  gs://k8-books-bucket/k8_books_clean.csv \
  --database=book_recommendations_d \
  --table=books \
  --columns="title,authors,publisher,published_date,description,isbn_10,isbn_13,reading_mode_text,reading_mode_image,page_count,categories,image_small,image_large,language,sale_country,list_price_amount,list_price_currency,buy_link,web_reader_link,embeddable,grade" \
  --project=book-recommendations-456120
```
10.  fixing issue
```
gsutil cp gs://k8-books-bucket/k8_books_clean.csv .
awk 'BEGIN{FS=OFS=","} NR==1{$1="id,"$1} NR>1{$1=","$1} 1' k8_books_clean.csv > k8_books_with_id.csv

gsutil cp k8_books_with_id.csv gs://k8-books-bucket/

```

# skip merging the tables and go back to fixing the front end
1. fix db.py (host was not connecting to right place)
```
import psycopg2
import os

def get_db_connection():
    return psycopg2.connect(
        dbname=os.environ["DB_NAME"],
        user=os.environ["DB_USER"],
        password=os.environ["DB_PASSWORD"],
        host=f"/cloudsql/{os.environ['INSTANCE_CONNECTION_NAME']}"  # Cloud SQL socket path
    )
```

2. rebuild and push Docker
```
docker build -t gcr.io/book-recommendations-456120/book-api .
docker push gcr.io/book-recommendations-456120/book-api
```

3. redeploy to Cloud Run
```
gcloud run deploy book-api \
  --image gcr.io/book-recommendations-456120/book-api \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --add-cloudsql-instances=book-recommendations-456120:us-central1:book-recs-db \
  --set-env-vars DB_NAME=book_recommendations_db,DB_USER=postgres,DB_PASSWORD=stimac-cis655-final,INSTANCE_CONNECTION_NAME=book-recommendations-456120:us-central1:book-recs-db \
  --port 8080
```
4. YAY it works now

5. update thumbs up
```
psql "host=127.0.0.1 dbname=book_recommendations_db user=postgres password=stimac-cis655-final"

ALTER TABLE books
ADD COLUMN IF NOT EXISTS thumbs_up INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS thumbs_down INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS avg_rating FLOAT;
```

6. add avg_rating to table: Open the BigQuery Console >> book-recommendations-456120 >> book_recs >> book_embeddings >> click "Edit schema" >> "Add field" >> Name: avg_rating Type: FLOAT Mode: NULLABLE >> Save.

7. create avg_rating data in colab: https://colab.research.google.com/drive/1xU3Ccc4pD0RzjutHEIZYyJdMI5q3IRkh?usp=sharing 

8. update index.html getMetadataRecommendations() function
```
div.innerHTML = `
  <img src="${book.image_small || '/static/placeholder.png'}" alt="cover">
  <div>
    <strong>${book.title}</strong><br>
    <em>${book.authors}</em><br>
    üëç ${book.thumbs_up || 0}‚ÄÉüëé ${book.thumbs_down || 0}‚ÄÉ‚≠ê ${book.avg_rating?.toFixed(1) || "N/A"}<br>
    <button onclick="rateBook('${book.isbn_10}', '${book.title}', '${book.authors}', 'like')">üëç</button>
    <button onclick="rateBook('${book.isbn_10}', '${book.title}', '${book.authors}', 'dislike')">üëé</button><br>
    ${book.web_reader_link ? `<a href="${book.web_reader_link}" target="_blank">Read</a>` : ''}
  </div>
`;
```

9. redeploy
```
docker build -t gcr.io/book-recommendations-456120/book-api .
docker push gcr.io/book-recommendations-456120/book-api

gcloud run deploy book-api \
  --image gcr.io/book-recommendations-456120/book-api \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --add-cloudsql-instances book-recommendations-456120:us-central1:book-recs-db \
  --set-env-vars DB_NAME=book_recommendations_db,DB_USER=postgres,DB_PASSWORD=stimac-cis655-final,INSTANCE_CONNECTION_NAME=book-recommendations-456120:us-central1:book-recs-db \
  --port 8080
```

10. fix clean_books_csv.py

11. fix fetch_books_dag.py

12. push to Composer
```
gsutil cp fetch_books_dag.py gs://us-central1-book-data-pipel-122da488-bucket/dags
```

13. create file to append book files to table append_books_from_gcs.py
```
insert append_books_from_gcs.py file
```

14. create main.py in book-api
```
from append_books_from_gcs import append_books
```

15. deploy
```
gcloud functions deploy append_books \
  --runtime python311 \
  --trigger-http \
  --allow-unauthenticated \
  --set-env-vars DB_NAME=book_recommendations_db,DB_USER=postgres,DB_PASSWORD=your_password,INSTANCE_CONNECTION_NAME=book-recommendations-456120:us-central1:book-recs-db \
  --entry-point append_books \
  --source .

```

16. get function url
```
gcloud functions describe append_books --gen2 --region=us-central1 --format="value(serviceConfig.uri)"
# example: https://append-books-uyxqxqnvtq-uc.a.run.app
```

17. 
